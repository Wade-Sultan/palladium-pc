"""
PC Build Recommender Pipeline  (Sequential Architecture)
=========================================================
LangGraph-based pipeline that recommends PC parts one at a time in
dependency order.  Each step:

  1. Queries a compatibility database to get valid options (TODO/stub).
  2. Calls the LLM with the filtered options + prior selections.
  3. Stores the chosen part in pipeline state.

Dependency chain
----------------
  CPU  →  CPU Cooler  →  Motherboard  →  RAM  →  Storage  →  GPU  →  PSU
    ↓
  Case  (presents 3 options, **pauses** for user selection)
    ↓
  Fans  (if needed)

Pricing
-------
Prices are NEVER generated by the LLM.  Each `PartRecommendation` carries
`price_usd`, `amazon_asin`, and `amazon_url` fields that start as None and
are populated by a downstream Amazon pricing pipeline (TODO).
`BuildRecommendation.compute_total_price()` returns a deterministic sum
only when every part has been priced.

Progress
--------
A caller-supplied `progress_callback(step_name, message)` is invoked at
the start of each node so the frontend can display live status updates
like "Choosing your CPU…".
"""

from __future__ import annotations

import os
from typing import Any, Callable, Literal

from dotenv import load_dotenv
from langchain_core.messages import HumanMessage, SystemMessage
from langgraph.graph import END, StateGraph
from pydantic import BaseModel, Field

load_dotenv()


# Structured Input

class UserPreferences(BaseModel):
    """Optional high-level preferences that sit outside the per-use-case Q&A."""
    preferred_brand_cpu: Literal["amd", "intel", "no_preference"] = "no_preference"
    preferred_brand_gpu: Literal["nvidia", "amd", "no_preference"] = "no_preference"
    form_factor: Literal["atx", "matx", "itx", "no_preference"] = "no_preference"
    rgb_lighting: bool = False
    wifi_required: bool = True
    color_theme: str | None = Field(None, description="e.g. 'black', 'white', 'black & red'")


class BuildRequest(BaseModel):
    """
    Structured input sent from the frontend Build Configurator.

    `answers` is a flat dict keyed by "<useCase>.<questionId>" whose values
    are either a single string (single-select) or a list of strings
    (multi-select), mirroring the React state.
    """
    use_cases: list[str] = Field(
        ...,
        description="Selected use-case keys: gaming, productivity, creative, streaming, aiml, nas",
    )
    preferences: UserPreferences = Field(default_factory=UserPreferences)
    answers: dict[str, str | list[str]] = Field(
        default_factory=dict,
        description="Flat map of '<useCase>.<questionId>' → answer(s)",
    )


# Output Schemas

class LLMPartPick(BaseModel):
    """Schema the LLM fills for a single component. No price fields exist."""
    name: str = Field(..., description="Full official product name, e.g. 'AMD Ryzen 7 7800X3D'")
    reason: str = Field(..., description="1-2 sentence justification for this choice")


class LLMMultiPartPick(BaseModel):
    """Schema for when the LLM returns multiple ranked options (used for case)."""
    options: list[LLMPartPick] = Field(
        ..., min_length=3, max_length=3,
        description="Exactly 3 compatible options, ranked by recommendation strength",
    )


# Final Output

class PartRecommendation(BaseModel):
    """A recommended component with pricing slots for downstream enrichment."""
    name: str = Field(..., description="Full product name")
    category: str = Field(..., description="Component category, e.g. 'CPU'")
    reason: str = Field(..., description="Why this part was chosen")
    # Pricing (populated by Amazon lookup, NOT by the LLM)
    price_usd: float | None = Field(None)
    amazon_url: str | None = Field(None)
    amazon_asin: str | None = Field(None)

    @classmethod
    def from_llm(cls, llm_pick: LLMPartPick, category: str) -> PartRecommendation:
        return cls(name=llm_pick.name, category=category, reason=llm_pick.reason)


class BuildRecommendation(BaseModel):
    """Complete build — assembled incrementally by the pipeline."""
    cpu: PartRecommendation
    cpu_cooler: PartRecommendation
    motherboard: PartRecommendation
    ram: PartRecommendation
    storage: PartRecommendation
    gpu: PartRecommendation | None = None
    case: PartRecommendation
    psu: PartRecommendation
    fans: PartRecommendation | None = None
    build_notes: str = ""

    # Deterministic pricing
    total_price_usd: float | None = None

    def compute_total_price(self) -> float | None:
        """Sum part prices. Returns None if ANY part is still unpriced."""
        parts = [self.cpu, self.cpu_cooler, self.motherboard, self.ram,
                 self.storage, self.case, self.psu]
        optional = [p for p in [self.gpu, self.fans] if p is not None]
        all_parts = parts + optional

        if any(p.price_usd is None for p in all_parts):
            return None

        self.total_price_usd = round(sum(p.price_usd for p in all_parts), 2)  # type: ignore[arg-type]
        return self.total_price_usd


# Pipeline State

class PipelineState(BaseModel):
    """
    Accumulates parts as each node runs.  This is the LangGraph state object.
    """
    model_config = {"arbitrary_types_allowed": True}

    # Inputs (set once at the start)
    request: BuildRequest
    progress_callback: Callable[[str, str], None] | None = Field(
        default=None, exclude=True,
        description="Optional callback(step_name, message) for live UI updates",
    )

    # Accumulated part picks (set one-by-one as nodes execute)
    cpu: LLMPartPick | None = None
    cpu_cooler: LLMPartPick | None = None
    motherboard: LLMPartPick | None = None
    ram: LLMPartPick | None = None
    storage: LLMPartPick | None = None
    gpu: LLMPartPick | None = None        # None means no discrete GPU needed
    psu: LLMPartPick | None = None

    # Case is special: 3 options presented, user picks one
    case_options: list[LLMPartPick] | None = None
    case_selection: LLMPartPick | None = None  # set after user picks

    fans: LLMPartPick | None = None        # None means case fans are sufficient

    build_notes: str = ""
    gpu_required: bool = True              # determined during the GPU step

    # Error tracking
    error: str | None = None


# LLM Provider

LLMProvider = Literal["openai", "anthropic"]


def _get_provider() -> LLMProvider:
    if os.getenv("OPENAI_API_KEY"):
        return "openai"
    if os.getenv("ANTHROPIC_API_KEY"):
        return "anthropic"
    raise EnvironmentError(
        "Set either OPENAI_API_KEY or ANTHROPIC_API_KEY in your environment."
    )


def _get_chat_model():
    """Return a raw LangChain chat model (no structured output binding)."""
    provider = _get_provider()
    if provider == "openai":
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(model="gpt-4o", temperature=0.3)
    else:
        from langchain_anthropic import ChatAnthropic
        return ChatAnthropic(model="claude-sonnet-4-20250514", temperature=0.3, max_tokens=4096)


def _call_llm_structured(system: str, user: str, schema: type[BaseModel]) -> BaseModel:
    """Call the LLM and parse the response into `schema`."""
    llm = _get_chat_model().with_structured_output(schema)
    return llm.invoke([SystemMessage(content=system), HumanMessage(content=user)])


# Helpers

def _emit_progress(state: PipelineState, step: str, message: str) -> None:
    if state.progress_callback:
        state.progress_callback(step, message)


def _format_request_context(req: BuildRequest) -> str:
    """Render the user's use-cases, preferences, and answers into a text block."""
    sections: list[str] = []

    sections.append(f"USE CASES: {', '.join(req.use_cases)}")

    pref = req.preferences
    pref_lines = [
        f"CPU brand: {pref.preferred_brand_cpu}",
        f"GPU brand: {pref.preferred_brand_gpu}",
        f"Form factor: {pref.form_factor}",
        f"RGB: {'yes' if pref.rgb_lighting else 'no'}",
        f"WiFi required: {'yes' if pref.wifi_required else 'no'}",
    ]
    if pref.color_theme:
        pref_lines.append(f"Color theme: {pref.color_theme}")
    sections.append("PREFERENCES: " + " | ".join(pref_lines))

    if req.answers:
        answer_lines = [
            f"  {k}: {', '.join(v) if isinstance(v, list) else v}"
            for k, v in sorted(req.answers.items())
        ]
        sections.append("QUESTIONNAIRE ANSWERS:\n" + "\n".join(answer_lines))

    return "\n\n".join(sections)


def _format_prior_selections(state: PipelineState) -> str:
    """Render parts chosen so far into a text block for LLM context."""
    lines: list[str] = []
    for label, pick in [
        ("CPU", state.cpu),
        ("CPU Cooler", state.cpu_cooler),
        ("Motherboard", state.motherboard),
        ("RAM", state.ram),
        ("Storage", state.storage),
        ("GPU", state.gpu),
        ("PSU", state.psu),
        ("Case", state.case_selection),
    ]:
        if pick is not None:
            lines.append(f"  {label}: {pick.name}")
    if not lines:
        return ""
    return "PARTS ALREADY SELECTED:\n" + "\n".join(lines)


# ╔═══════════════════════════════════════════════════════════════════════════╗
# ║  COMPATIBILITY DB STUBS                                                  ║
# ║                                                                          ║
# ║  Each function below will eventually query your parts database and       ║
# ║  return a filtered list of compatible products.  For now they return     ║
# ║  None (meaning "no filter — let the LLM pick freely").                   ║
# ╚═══════════════════════════════════════════════════════════════════════════╝

def _db_get_compatible_cpus(request: BuildRequest) -> list[str] | None:
    """TODO: Query DB for CPUs matching use-case requirements + brand preference."""
    return None


def _db_get_compatible_cpu_coolers(
    cpu: LLMPartPick, form_factor: str, request: BuildRequest,
) -> list[str] | None:
    """TODO: Filter by CPU socket, TDP (air vs AIO), form factor clearance."""
    return None


def _db_get_compatible_motherboards(
    cpu: LLMPartPick, form_factor: str, request: BuildRequest,
) -> list[str] | None:
    """TODO: Filter by CPU socket, form factor, WiFi, feature set."""
    return None


def _db_get_compatible_ram(
    motherboard: LLMPartPick, request: BuildRequest,
) -> list[str] | None:
    """TODO: Filter by DDR type supported by the motherboard, capacity needs."""
    return None


def _db_get_compatible_storage(
    motherboard: LLMPartPick, request: BuildRequest,
) -> list[str] | None:
    """TODO: Filter by M.2 slots, interface (NVMe/SATA), capacity needs."""
    return None


def _db_get_compatible_gpus(request: BuildRequest) -> list[str] | None:
    """TODO: Filter by use-case VRAM needs, physical size constraints."""
    return None


def _db_get_compatible_psus(
    cpu: LLMPartPick, gpu: LLMPartPick | None, request: BuildRequest,
) -> list[str] | None:
    """TODO: Calculate total TDP + headroom, filter by wattage & efficiency."""
    return None


def _db_get_compatible_cases(
    motherboard: LLMPartPick, cpu_cooler: LLMPartPick,
    gpu: LLMPartPick | None, request: BuildRequest,
) -> list[str] | None:
    """TODO: Filter by form factor, GPU clearance, cooler height/radiator support."""
    return None


def _db_get_compatible_fans(
    case: LLMPartPick, request: BuildRequest,
) -> list[str] | None:
    """TODO: Check case's included fans vs thermal needs, return options if needed."""
    return None


# ╔═══════════════════════════════════════════════════════════════════════════╗
# ║  PER-COMPONENT SYSTEM PROMPTS                                           ║
# ╚═══════════════════════════════════════════════════════════════════════════╝

_BASE_RULES = """\
You are an expert PC hardware advisor selecting ONE component for a build.

RULES:
- Pick a real, currently-available product. Use the full official product name.
- The part MUST be compatible with all previously-selected parts.
- Prioritize price-to-performance for the user's use cases.
- Do NOT mention prices — pricing is handled by a separate system.
- Give a concise 1-2 sentence reason for your choice.
"""

_SYSTEM_PROMPTS: dict[str, str] = {
    "cpu": _BASE_RULES + """
You are choosing the CPU. Consider the user's primary workloads and brand
preference.  If no discrete GPU will be needed (pure productivity / NAS),
pick a CPU with integrated graphics.
""",

    "cpu_cooler": _BASE_RULES + """
You are choosing the CPU cooler. Consider:
- The CPU's socket type and TDP.
- If CPU TDP is ≤ 65W, a quality air cooler is fine.
- If CPU TDP is > 65W and ≤ 125W, recommend a good tower air cooler or 240mm AIO.
- If CPU TDP is > 125W, recommend a 280mm or 360mm AIO.
- The build's form factor (ITX builds need low-profile or small AIO coolers).
""",

    "motherboard": _BASE_RULES + """
You are choosing the motherboard. It MUST match:
- The CPU's socket type exactly.
- The desired form factor (ATX / mATX / ITX).
- Include WiFi if the user requires it.
Consider VRM quality for the CPU's power draw.
""",

    "ram": _BASE_RULES + """
You are choosing the RAM kit. It MUST be the DDR generation supported by the
chosen motherboard.  Consider capacity needs based on the user's workloads:
- Productivity / Gaming: 32 GB is the sweet spot.
- Creative / Streaming: 32-64 GB.
- AI/ML or heavy creative: 64-128 GB.
Pick a speed that the motherboard and CPU's memory controller handle well.
""",

    "storage": _BASE_RULES + """
You are choosing the primary storage drive. Prefer NVMe Gen4 M.2 SSDs for
the boot drive.  Match capacity to the user's needs from their questionnaire
answers.  Make sure the motherboard has a compatible M.2 slot.
""",

    "gpu": _BASE_RULES + """
You are choosing the GPU.  If the build genuinely does not need a discrete
GPU (e.g. office productivity, NAS, or the CPU has sufficient integrated
graphics for the workload), respond with a pick where name is "NONE" and
reason explains why integrated graphics suffice.

Otherwise pick the best GPU for the user's workloads and brand preference.
Consider VRAM requirements for AI/ML and creative workloads.
""",

    "psu": _BASE_RULES + """
You are choosing the power supply (PSU).  Calculate approximate total system
power draw from the selected CPU and GPU, then add 20-30% headroom.
- Pick 80+ Gold or better efficiency.
- Must be the right form factor (ATX, SFX for ITX builds).
- Ensure it has the right PCIe power connectors for the GPU.
""",

    "case": _BASE_RULES + """
You are choosing PC cases.  Pick exactly 3 options that are ALL compatible
with the selected parts:
- Must fit the motherboard form factor.
- Must have enough GPU clearance for the selected GPU (if any).
- Must support the selected CPU cooler (tower height or radiator size).
- Respect the user's color/aesthetic preferences.

Rank them: #1 = your top recommendation, #2 = solid alternative, #3 = budget-friendly option.
""",

    "fans": _BASE_RULES + """
You are deciding if additional case fans are needed and, if so, which ones.

If the selected case comes with sufficient fans for adequate airflow given
the build's thermal output, respond with a pick where name is "NONE" and
reason explains that the included fans are sufficient.

Otherwise recommend a fan kit (e.g. a 3-pack of 120mm or 140mm fans).
""",
}


# ╔═══════════════════════════════════════════════════════════════════════════╗
# ║  GRAPH NODES                                                             ║
# ╚═══════════════════════════════════════════════════════════════════════════╝

def _build_user_prompt(
    state: PipelineState,
    component: str,
    compatible_options: list[str] | None = None,
) -> str:
    """Assemble the user-turn prompt for a single-component LLM call."""
    parts: list[str] = []

    parts.append(_format_request_context(state.request))

    prior = _format_prior_selections(state)
    if prior:
        parts.append(prior)

    if compatible_options:
        parts.append(
            f"COMPATIBLE {component.upper()} OPTIONS (you MUST pick from this list):\n"
            + "\n".join(f"  - {opt}" for opt in compatible_options)
        )

    parts.append(f"Now select the {component}.")
    return "\n\n".join(parts)


# ---- Step 1: CPU ----

def pick_cpu(state: PipelineState) -> dict:
    _emit_progress(state, "cpu", "Choosing your CPU…")
    try:
        options = _db_get_compatible_cpus(state.request)
        prompt = _build_user_prompt(state, "CPU", options)
        pick = _call_llm_structured(_SYSTEM_PROMPTS["cpu"], prompt, LLMPartPick)
        return {"cpu": pick}
    except Exception as exc:
        return {"error": f"CPU selection failed: {exc}"}


# ---- Step 2: CPU Cooler ----

def pick_cpu_cooler(state: PipelineState) -> dict:
    _emit_progress(state, "cpu_cooler", "Selecting a CPU cooler…")
    try:
        form_factor = state.request.preferences.form_factor
        options = _db_get_compatible_cpu_coolers(state.cpu, form_factor, state.request)
        prompt = _build_user_prompt(state, "CPU cooler", options)
        pick = _call_llm_structured(_SYSTEM_PROMPTS["cpu_cooler"], prompt, LLMPartPick)
        return {"cpu_cooler": pick}
    except Exception as exc:
        return {"error": f"CPU cooler selection failed: {exc}"}


# ---- Step 3: Motherboard ----

def pick_motherboard(state: PipelineState) -> dict:
    _emit_progress(state, "motherboard", "Finding the right motherboard…")
    try:
        form_factor = state.request.preferences.form_factor
        options = _db_get_compatible_motherboards(state.cpu, form_factor, state.request)
        prompt = _build_user_prompt(state, "motherboard", options)
        pick = _call_llm_structured(_SYSTEM_PROMPTS["motherboard"], prompt, LLMPartPick)
        return {"motherboard": pick}
    except Exception as exc:
        return {"error": f"Motherboard selection failed: {exc}"}


# ---- Step 4: RAM ----

def pick_ram(state: PipelineState) -> dict:
    _emit_progress(state, "ram", "Picking your memory…")
    try:
        options = _db_get_compatible_ram(state.motherboard, state.request)
        prompt = _build_user_prompt(state, "RAM", options)
        pick = _call_llm_structured(_SYSTEM_PROMPTS["ram"], prompt, LLMPartPick)
        return {"ram": pick}
    except Exception as exc:
        return {"error": f"RAM selection failed: {exc}"}


# ---- Step 5: Storage ----

def pick_storage(state: PipelineState) -> dict:
    _emit_progress(state, "storage", "Choosing your storage…")
    try:
        options = _db_get_compatible_storage(state.motherboard, state.request)
        prompt = _build_user_prompt(state, "storage drive", options)
        pick = _call_llm_structured(_SYSTEM_PROMPTS["storage"], prompt, LLMPartPick)
        return {"storage": pick}
    except Exception as exc:
        return {"error": f"Storage selection failed: {exc}"}


# ---- Step 6: GPU ----

def pick_gpu(state: PipelineState) -> dict:
    _emit_progress(state, "gpu", "Selecting a graphics card…")
    try:
        options = _db_get_compatible_gpus(state.request)
        prompt = _build_user_prompt(state, "GPU", options)
        pick = _call_llm_structured(_SYSTEM_PROMPTS["gpu"], prompt, LLMPartPick)

        if pick.name.upper() == "NONE":
            return {"gpu": None, "gpu_required": False}
        return {"gpu": pick, "gpu_required": True}
    except Exception as exc:
        return {"error": f"GPU selection failed: {exc}"}


# ---- Step 7: PSU ----

def pick_psu(state: PipelineState) -> dict:
    _emit_progress(state, "psu", "Sizing your power supply…")
    try:
        options = _db_get_compatible_psus(state.cpu, state.gpu, state.request)
        prompt = _build_user_prompt(state, "power supply (PSU)", options)
        pick = _call_llm_structured(_SYSTEM_PROMPTS["psu"], prompt, LLMPartPick)
        return {"psu": pick}
    except Exception as exc:
        return {"error": f"PSU selection failed: {exc}"}


# ---- Step 8: Case (3 options → pause for user) ----

def pick_case_options(state: PipelineState) -> dict:
    _emit_progress(state, "case", "Finding compatible cases for you to choose from…")
    try:
        options = _db_get_compatible_cases(
            state.motherboard, state.cpu_cooler, state.gpu, state.request,
        )
        prompt = _build_user_prompt(state, "case", options)
        prompt += "\n\nProvide exactly 3 case options."
        multi = _call_llm_structured(_SYSTEM_PROMPTS["case"], prompt, LLMMultiPartPick)
        return {"case_options": multi.options}
    except Exception as exc:
        return {"error": f"Case selection failed: {exc}"}


def await_case_selection(state: PipelineState) -> dict:
    """
    Human-in-the-loop node.

    In production this is an *interrupt* point.  The graph pauses here, the
    API returns the 3 case options to the frontend, and the graph resumes
    when the user POSTs their choice.

    When the graph resumes, `state.case_selection` will already be set by
    the caller (the API endpoint injects it before calling graph.resume()).

    If running locally / in tests with no interrupt support, the node
    auto-selects option #1 as a fallback.
    """
    if state.case_selection is not None:
        # User already made their pick (set externally before resume)
        return {}

    # Fallback: auto-pick #1 (for local testing only)
    if state.case_options:
        return {"case_selection": state.case_options[0]}

    return {"error": "No case options available to select from."}


# ---- Step 9: Fans ----

def pick_fans(state: PipelineState) -> dict:
    _emit_progress(state, "fans", "Checking if you need extra fans…")
    try:
        options = _db_get_compatible_fans(state.case_selection, state.request)
        prompt = _build_user_prompt(state, "case fans", options)
        pick = _call_llm_structured(_SYSTEM_PROMPTS["fans"], prompt, LLMPartPick)

        if pick.name.upper() == "NONE":
            return {"fans": None}
        return {"fans": pick}
    except Exception as exc:
        return {"error": f"Fan selection failed: {exc}"}


# ---- Final assembly ----

def assemble_build(state: PipelineState) -> dict:
    """Collect all picks into the final BuildRecommendation."""
    _emit_progress(state, "done", "Finalizing your build…")
    return {"build_notes": "Build assembled successfully."}


# ╔═══════════════════════════════════════════════════════════════════════════╗
# ║  CONDITIONAL EDGES                                                       ║
# ╚═══════════════════════════════════════════════════════════════════════════╝

def _check_error(state: PipelineState) -> str:
    """After every node: bail out to END if an error occurred."""
    return "error" if state.error else "continue"


# ╔═══════════════════════════════════════════════════════════════════════════╗
# ║  GRAPH CONSTRUCTION                                                      ║
# ╚═══════════════════════════════════════════════════════════════════════════╝

def _build_graph() -> Any:
    g = StateGraph(PipelineState)

    # --- Add nodes ---
    nodes = [
        ("pick_cpu",             pick_cpu),
        ("pick_cpu_cooler",      pick_cpu_cooler),
        ("pick_motherboard",     pick_motherboard),
        ("pick_ram",             pick_ram),
        ("pick_storage",         pick_storage),
        ("pick_gpu",             pick_gpu),
        ("pick_psu",             pick_psu),
        ("pick_case_options",    pick_case_options),
        ("await_case_selection", await_case_selection),
        ("pick_fans",            pick_fans),
        ("assemble_build",       assemble_build),
    ]
    for name, fn in nodes:
        g.add_node(name, fn)

    # --- Entry point ---
    g.set_entry_point("pick_cpu")

    # --- Sequential edges with error checking after each step ---
    step_order = [
        "pick_cpu",
        "pick_cpu_cooler",
        "pick_motherboard",
        "pick_ram",
        "pick_storage",
        "pick_gpu",
        "pick_psu",
        "pick_case_options",
        "await_case_selection",
        "pick_fans",
        "assemble_build",
    ]

    for i, step in enumerate(step_order[:-1]):
        next_step = step_order[i + 1]
        g.add_conditional_edges(
            step,
            _check_error,
            {"continue": next_step, "error": END},
        )

    # Final node → END
    g.add_edge("assemble_build", END)

    # --- Compile with interrupt before case selection (human-in-the-loop) ---
    return g.compile(interrupt_before=["await_case_selection"])


# Compile once at module level
_pipeline = _build_graph()


# ╔═══════════════════════════════════════════════════════════════════════════╗
# ║  PUBLIC API                                                              ║
# ╚═══════════════════════════════════════════════════════════════════════════╝

def recommend_build_phase1(
    request: BuildRequest,
    progress_callback: Callable[[str, str], None] | None = None,
) -> dict:
    """
    Run Phase 1: CPU through PSU + generate 3 case options, then pause.

    Returns
    -------
    dict with keys:
        "case_options" : list of 3 dicts with "name" and "reason"
        "thread_state" : opaque state needed to resume the pipeline
        "error"        : str or None
    """
    initial = PipelineState(
        request=request,
        progress_callback=progress_callback,
    )
    config = {"configurable": {"thread_id": "build-session"}}

    state = _pipeline.invoke(initial, config=config)

    if state.get("error"):
        return {"case_options": [], "thread_state": None, "error": state["error"]}

    case_options = state.get("case_options", [])
    return {
        "case_options": [
            {"name": opt.name, "reason": opt.reason}
            for opt in (case_options or [])
        ],
        "thread_state": config,
        "error": None,
    }


def recommend_build_phase2(
    thread_state: dict,
    selected_case_index: int,
    progress_callback: Callable[[str, str], None] | None = None,
) -> BuildRecommendation:
    """
    Resume the pipeline after the user selects one of the 3 cases.

    Parameters
    ----------
    thread_state       : opaque state from phase 1
    selected_case_index: 0, 1, or 2
    progress_callback  : optional UI callback

    Returns
    -------
    BuildRecommendation with all parts selected (prices still None).
    """
    current = _pipeline.get_state(thread_state)
    case_options = current.values.get("case_options", [])

    if not case_options or selected_case_index not in (0, 1, 2):
        raise ValueError("Invalid case selection. Must be 0, 1, or 2.")

    _pipeline.update_state(
        thread_state,
        {
            "case_selection": case_options[selected_case_index],
            "progress_callback": progress_callback,
        },
    )

    state = _pipeline.invoke(None, config=thread_state)

    if state.get("error"):
        raise RuntimeError(f"Recommendation failed: {state['error']}")

    return BuildRecommendation(
        cpu=PartRecommendation.from_llm(state["cpu"], "CPU"),
        cpu_cooler=PartRecommendation.from_llm(state["cpu_cooler"], "CPU Cooler"),
        motherboard=PartRecommendation.from_llm(state["motherboard"], "Motherboard"),
        ram=PartRecommendation.from_llm(state["ram"], "RAM"),
        storage=PartRecommendation.from_llm(state["storage"], "Storage"),
        gpu=PartRecommendation.from_llm(state["gpu"], "GPU") if state.get("gpu") else None,
        case=PartRecommendation.from_llm(state["case_selection"], "Case"),
        psu=PartRecommendation.from_llm(state["psu"], "PSU"),
        fans=PartRecommendation.from_llm(state["fans"], "Fans") if state.get("fans") else None,
        build_notes=state.get("build_notes", ""),
    )


# ╔═══════════════════════════════════════════════════════════════════════════╗
# ║  SMOKE TEST                                                              ║
# ╚═══════════════════════════════════════════════════════════════════════════╝

if __name__ == "__main__":
    def _progress(step: str, msg: str) -> None:
        print(f"  [{step}] {msg}")

    sample = BuildRequest(
        use_cases=["gaming", "streaming"],
        preferences=UserPreferences(
            preferred_brand_cpu="amd",
            preferred_brand_gpu="nvidia",
            form_factor="atx",
            rgb_lighting=False,
            wifi_required=True,
        ),
        answers={
            "gaming.resolution": "1440p (Quad HD)",
            "gaming.fps": "144+ fps (Competitive)",
            "gaming.gameTypes": [
                "AAA Open World (Cyberpunk, Elden Ring)",
                "Competitive FPS (Valorant, CS2)",
            ],
            "gaming.raytracing": "Nice to have",
            "streaming.platform": ["Twitch", "YouTube"],
            "streaming.streamQuality": "1080p 60fps",
            "streaming.encoding": "GPU encoding (NVENC / AMF)",
            "streaming.multitask": "Yes, always",
        },
    )

    print("=" * 60)
    print("  PC Build Recommender — Sequential Pipeline Smoke Test")
    print("=" * 60)

    try:
        print("\n--- Phase 1: Selecting components ---")
        result1 = recommend_build_phase1(sample, progress_callback=_progress)

        if result1["error"]:
            print(f"\nError: {result1['error']}")
        else:
            print("\n--- Case options for user ---")
            for i, opt in enumerate(result1["case_options"]):
                print(f"  [{i}] {opt['name']}: {opt['reason']}")

            print("\n--- Phase 2: Auto-selecting case #0 and finishing ---")
            final = recommend_build_phase2(
                result1["thread_state"],
                selected_case_index=0,
                progress_callback=_progress,
            )
            print("\n--- Final Build ---")
            print(final.model_dump_json(indent=2))

    except Exception as e:
        print(f"\nError (expected if no API key set): {e}")

        # Show what a single prompt would look like
        print("\n--- Sample prompt for CPU step ---")
        test_state = PipelineState(request=sample)
        print(_build_user_prompt(test_state, "CPU"))